{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhamct/Software-Requirement-Classification/blob/main/Software_Requirement_Classification_Vietnamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrFIlT9gy6uv"
      },
      "source": [
        "# Bài toán:\n",
        "Input: tập dữ liệu tiếng Việt <br>\n",
        "Output: phân loại (yêu cầu chức năng, yêu cầu phi chức năng) <br>\n",
        "Process: sử dụng bộ thư viện Scikitlearn để thực hiện các thuật toán học máy có giám sát:\n",
        "*   Logistic Regression\n",
        "*   Naive Bayes\n",
        "*   Support Vector Machine\n",
        "Nếu còn thời gian, thực hiện thuật toán Fasttext (https://fasttext.cc/) của Facebook\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjGdHmUu1ONy"
      },
      "source": [
        "# Tiền xử lý\n",
        "\n",
        "\n",
        "1.   Xóa HTML code(nếu có)\n",
        "2.   Chuẩn hóa bảng mã UNICODE (đưa về UNICODE tổ hợp dựng sẵn)\n",
        "3.   Chuẩn hóa kiểu gõ dấu tiếng Việt\n",
        "4.   Tách từ tiếng Việt\n",
        "5.   Đưa về văn bản viết thường\n",
        "6. Xóa kí tự đặc biệt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Anjukome2dYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8ed2b7-4d32-4c1b-b37e-9b2c0745d4fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-1.3.4-py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.2)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.1.0)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Collecting underthesea-core==0.0.4_alpha.10\n",
            "  Downloading underthesea_core-0.0.4_alpha.10-cp37-cp37m-manylinux2010_x86_64.whl (581 kB)\n",
            "\u001b[K     |████████████████████████████████| 581 kB 50.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.63.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.21.5)\n",
            "Installing collected packages: unidecode, underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.8 underthesea-1.3.4 underthesea-core-0.0.4a10 unidecode-1.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "neekoSM843vD"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "from underthesea import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hbtYnHmT237c"
      },
      "outputs": [],
      "source": [
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        " \n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
        "        '|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "dicchar = loaddicchar()\n",
        "\n",
        "# Hàm chuyển Unicode dựng sẵn về Unicde tổ hợp (phổ biến hơn)\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "        lambda x: dicchar[x.group()], txt)\n",
        "\n",
        "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
        "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
        "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
        "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
        "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
        "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
        "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
        "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
        "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
        "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
        "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
        "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
        "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
        "\n",
        "nguyen_am_to_ids = {}\n",
        "\n",
        "for i in range(len(bang_nguyen_am)):\n",
        "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
        "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
        "\n",
        "def chuan_hoa_dau_tu_tieng_viet(word):\n",
        "    if not is_valid_vietnam_word(word):\n",
        "        return word\n",
        "\n",
        "    chars = list(word)\n",
        "    dau_cau = 0\n",
        "    nguyen_am_index = []\n",
        "    qu_or_gi = False\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9:  # check qu\n",
        "            if index != 0 and chars[index - 1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5:  # check gi\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "            chars[index] = bang_nguyen_am[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            nguyen_am_index.append(index)\n",
        "    if len(nguyen_am_index) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = nguyen_am_to_ids.get(chars[1])\n",
        "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
        "            else:\n",
        "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
        "                else:\n",
        "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
        "            return ''.join(chars)\n",
        "        return word\n",
        "\n",
        "    for index in nguyen_am_index:\n",
        "        x, y = nguyen_am_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8:  # ê, ơ\n",
        "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
        "            # for index2 in nguyen_am_index:\n",
        "            #     if index2 != index:\n",
        "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
        "            #         chars[index2] = bang_nguyen_am[x][0]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(nguyen_am_index) == 2:\n",
        "        if nguyen_am_index[-1] == len(chars) - 1:\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
        "        else:\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    else:\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
        "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def is_valid_vietnam_word(word):\n",
        "    chars = list(word)\n",
        "    nguyen_am_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if nguyen_am_index == -1:\n",
        "                nguyen_am_index = index\n",
        "            else:\n",
        "                if index - nguyen_am_index != 1:\n",
        "                    return False\n",
        "                nguyen_am_index = index\n",
        "    return True\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
        "    \"\"\"\n",
        "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
        "        :param sentence:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
        "        # print(cw)\n",
        "        if len(cw) == 3:\n",
        "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
        "        words[index] = ''.join(cw)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def remove_html(txt):\n",
        "    return re.sub(r'<[^>]*>', '', txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZO7eXEJYQaku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60948cf3-dc79-48b7-c04c-c173ee030266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tất_cả các thiết_bị của doanh_nghiệp được quản_lý bằng cách tích_hợp code bar code qr code rfid lưu_trữ toàn_bộ thông_tin mã_hóa của từng thiết_bị được ghi_nhận ở các trạng_thái đang hoạt_động không hoạt_động trên hệ_thống\n"
          ]
        }
      ],
      "source": [
        "def text_preprocess(document):\n",
        "    # xóa html code\n",
        "    document = remove_html(document)\n",
        "    # chuẩn hóa unicode\n",
        "    document = convert_unicode(document)\n",
        "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
        "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
        "    # tách từ\n",
        "    document = word_tokenize(document, format=\"text\")\n",
        "    # đưa về lower\n",
        "    document = document.lower()\n",
        "    # xóa các ký tự không cần thiết\n",
        "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
        "    # xóa khoảng trắng thừa\n",
        "    document = re.sub(r'\\s+', ' ', document).strip()\n",
        "    return document\n",
        "\n",
        "document = \"\"\"\n",
        "Tất cả các thiết bị của doanh nghiệp, được quản lý bằng cách tích hợp code (bar code/ QR code/ RFID,...) lưu trữ toàn bộ thông tin mã hóa của từng thiết bị, được ghi nhận ở các trạng thái Đang hoạt động/ Không hoạt động trên hệ thống. \n",
        "\"\"\"\n",
        "document = text_preprocess(document)\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99OoJx_rv8n"
      },
      "source": [
        "## PHÂN LOẠI TEXT\n",
        "* Tải tập dữ liệu đã tiền xử lý về\n",
        "\n",
        "* Quan sát tập dữ liệu\n",
        "* Loại bỏ stopword\n",
        "* Xây dựng tập train/test\n",
        "* Phân loại văn bản sử dụng thuật toán Naive Bayes\n",
        "* Phân loại văn bản sử dụng thuật toán SVM\n",
        "* Phân loại văn bản sử dụng Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRswJ-KNr__X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ec73a9-c230-4fa4-d8a6-35e2ba1b2bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘dataset.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  dataset.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of dataset.zip or\n",
            "        dataset.zip.zip, and cannot find dataset.zip.ZIP, period.\n",
            "head: cannot open 'dataset.txt' for reading: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!wget -nc \"https://github.com/nhamct/Software-Requirement-Classification/blob/main/dataset.zip\"\n",
        "!unzip -n \"dataset.zip\"\n",
        "!head \"dataset.txt\"\n",
        "#cần zip file lại --> download về --> unzip thì mới ko bị lỗi HTML tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RpEdiNlBsm_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e6b6f1-9450-437b-a42f-4476e37e7679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__label__FR 131\n",
            "__label__NF 91\n"
          ]
        }
      ],
      "source": [
        "# Thống kê số lượng data theo nhãn\n",
        "count = {}\n",
        "for line in open('dataset.txt'):\n",
        "    key = line.split()[0]\n",
        "    count[key] = count.get(key, 0) + 1\n",
        "\n",
        "for key in count:\n",
        "    print(key, count[key])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0JMswMOep3Dh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F = (131)\n",
        "NF = (91)\n",
        "\n",
        "data = {'Functional': 98, 'Non-functional': 27}\n",
        "names = list(data.keys())\n",
        "values = list(data.values())\n",
        "\n",
        "fig, axs = plt.subplots(1, 1, figsize=(3, 3), sharey=True)\n",
        "axs.bar(names, values)\n",
        "\n",
        "#fig.suptitle('Statistics of requirements by functional & non-functional')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "vvWC8fqzp4I8",
        "outputId": "beda779a-334b-48a6-c00c-932b5f979616"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c5WEu7P1syOZ"
      },
      "outputs": [],
      "source": [
        "# Thống kê các word xuất hiện ở tất cả các nhãn\n",
        "total_label = 18\n",
        "vocab = {}\n",
        "label_vocab = {}\n",
        "for line in open('dataset.txt'):\n",
        "    words = line.split()\n",
        "    # lưu ý từ đầu tiên là nhãn\n",
        "    label = words[0]\n",
        "    if label not in label_vocab:\n",
        "        label_vocab[label] = {}\n",
        "    for word in words[1:]:\n",
        "        label_vocab[label][word] = label_vocab[label].get(word, 0) + 1\n",
        "        if word not in vocab:\n",
        "            vocab[word] = set()\n",
        "        vocab[word].add(label)\n",
        "\n",
        "count = {}\n",
        "for word in vocab:\n",
        "    if len(vocab[word]) == total_label:\n",
        "        count[word] = min([label_vocab[x][word] for x in label_vocab])\n",
        "        \n",
        "sorted_count = sorted(count, key=count.get, reverse=True)\n",
        "for word in sorted_count[:100]:\n",
        "    print(word, count[word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NGuQ9k69s_6J"
      },
      "outputs": [],
      "source": [
        "# loại stopword khỏi dữ liệu\n",
        "# lưu file dùng về sau\n",
        "stopword = set()\n",
        "with open('stopwords.txt', 'w') as fp:\n",
        "    for word in sorted_count[:100]:\n",
        "        stopword.add(word)\n",
        "        fp.write(word + '\\n')\n",
        "    \n",
        "def remove_stopwords(line):\n",
        "    words = []\n",
        "    for word in line.strip().split():\n",
        "        if word not in stopword:\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "    \n",
        "    \n",
        "with open('dataset.prep', 'w') as fp:\n",
        "    for line in open('dataset.txt'):\n",
        "        line = remove_stopwords(line)\n",
        "        fp.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w3nEh7rQtD7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57943e77-81fe-4534-aed9-1e05c5b1dbeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__label__FR', '__label__NF'] \n",
            "\n",
            "Line Manager tạo Hướng dẫn công việc trên hệ thống 0 \n",
            "\n",
            "Sản phẩm sẽ sử dụng menu điều hướng tiêu chuẩn quen thuộc với hầu hết người dùng web. 1\n"
          ]
        }
      ],
      "source": [
        "# Chia tập train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "test_percent = 0.2\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "\n",
        "for line in open('dataset.prep'):\n",
        "    words = line.strip().split()\n",
        "    label.append(words[0])\n",
        "    text.append(' '.join(words[1:]))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=42)\n",
        "\n",
        "# Lưu train/test data\n",
        "# Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
        "with open('train.txt', 'w') as fp:\n",
        "    for x, y in zip(X_train, y_train):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "with open('test.txt', 'w') as fp:\n",
        "    for x, y in zip(X_test, y_test):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "# encode label\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "print(list(label_encoder.classes_), '\\n')\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "print(X_train[0], y_train[0], '\\n')\n",
        "print(X_test[0], y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WGVKaWIvthdb"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"models\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    os.makedirs(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhYnb4kztp2-"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7thMfaa7tnvn",
        "outputId": "a78c0ef2-272b-4f24-9ae6-13695a1d6b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Naive Bayes in 0.017238616943359375 seconds.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Naive Bayes in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"naive_bayes.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oESE0P0qt0SH"
      },
      "source": [
        "### Logistics Regression Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW-PQ-eKt1XJ",
        "outputId": "705555e3-d044-4291-fbfe-92693b8fe244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Linear Classifier in 0.02591538429260254 seconds.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "    \n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression(solver='lbfgs', \n",
        "                                                multi_class='auto',\n",
        "                                                max_iter=10000))\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Linear Classifier in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"linear_classifier.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kek4R0ZuZEu"
      },
      "source": [
        "### Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g5jOTkMubHL",
        "outputId": "b2ab7cab-2157-4f0a-c91c-935e08e62558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training SVM in 0.01683640480041504 seconds.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SVC(gamma='scale'))\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training SVM in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"svm.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP"
      ],
      "metadata": {
        "id": "DlXoCBBFytsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "VGc0K4JmywuQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1))\n",
        "                    ])\n",
        "\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "train_time = time.time() - start_time\n",
        "print('Done training RNN in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"rnn.pkl\"), 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RfpC6z0yxal",
        "outputId": "b390ec6f-bb71-4ae4-d277-71f4852905b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training RNN in 0.015318870544433594 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random forest"
      ],
      "metadata": {
        "id": "GJw9_okPtMdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "_athUrYZtO2L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', RandomForestClassifier(max_depth=2, random_state=0))\n",
        "                    ])\n",
        "\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Random forest in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"ran.pkl\"), 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW14vmY5toMe",
        "outputId": "cf5499fe-aecf-4f34-904a-6766c3d108b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Random forest in 0.14330506324768066 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDfwJDfTu5C6"
      },
      "source": [
        "### FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtlaHQMXu6ST",
        "outputId": "81f2aab1-e327-4f78-a547-fa4baa69d634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed pandas-1.1.5\n",
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=a24f3dfda32a03a83ffb6cdce6b7bebc6eb43e26bc12b6351f4a600973911885\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting fasttext==0.9.2\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.21.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3148732 sha256=b16e97a95027884f30b41b75aa585aed9428e5be238218cff0ee95022b0115f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n"
          ]
        }
      ],
      "source": [
        "# Cài đặt fastText cho Python\n",
        "!pip install pandas==1.1.5\n",
        "!pip install wget==3.2\n",
        "!pip install fasttext==0.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "asW4bHSAu9MS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7284df4-3bfa-48cb-8012-e7fba3b69ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Fasttext in 1.564793586730957 seconds.\n"
          ]
        }
      ],
      "source": [
        "import fasttext\n",
        "\n",
        "start_time = time.time()\n",
        "model = fasttext.train_supervised(\n",
        "                                input='train.txt',\n",
        "                                dim=100,\n",
        "                                epoch=5,\n",
        "                                lr=0.1,\n",
        "                                wordNgrams=2,\n",
        "                                label='__label__',\n",
        "                                minCount=5\n",
        ")\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Fasttext in', train_time, 'seconds.')\n",
        "\n",
        "# Compress model files with quantization\n",
        "model.quantize(input='train.txt', retrain=True)\n",
        "model.save_model(os.path.join(MODEL_PATH,\"fasttext.ftz\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaxdhWu_vAyE"
      },
      "source": [
        "## Đánh giá các mô hình"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gCT3TzPIDA14"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sLrghLnvDXS",
        "outputId": "88a7478a-b410-4cfb-fde9-bf5bf70ddc91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random forest, Accuracy = 0.7555555555555555\n",
            "Random forest - precision: 0.8071428571428572\n",
            "Random forest - F1: 0.7201808931599774\n",
            "Random forest - recall: 0.7176113360323887\n",
            "Naive Bayes, Accuracy = 0.9333333333333333\n",
            "Naive - precision: 0.9299999999999999\n",
            "Naive - F1: 0.9321266968325792\n",
            "Naive - recall: 0.9352226720647774\n",
            "Logistics Classifier, Accuracy = 0.9111111111111111\n",
            "Logistics - precision: 0.9077380952380952\n",
            "Logis - F1: 0.9099999999999999\n",
            "LOgis - recall: 0.915991902834008\n",
            "SVM, Accuracy = 0.8888888888888888\n",
            "SVM - precision:  0.8958333333333333\n",
            "SVM - F1: 0.8886689757545769\n",
            "SVM - recall: 0.9038461538461539\n",
            "RNN, Accuracy = 0.5777777777777777\n",
            "RNN- precision: 0.28888888888888886\n",
            "RNN - F1: 0.3661971830985915\n",
            "RNN - recall: 0.5\n",
            "Fasttext, Precision = 0.7555555555555555, Recall = 0.7555555555555555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "# Random forest\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"ran.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('Random forest, Accuracy =', np.mean(y_pred == y_test))\n",
        "print ('Random forest - precision:', precision_score(y_test, y_pred, average='macro'))\n",
        "print ('Random forest - F1:', f1_score(y_test, y_pred, average='macro'))\n",
        "print ('Random forest - recall:', recall_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "# Naive Bayes\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('Naive Bayes, Accuracy =', np.mean(y_pred == y_test))\n",
        "print ('Naive - precision:', precision_score(y_test, y_pred, average='macro'))\n",
        "print ('Naive - F1:', f1_score(y_test, y_pred, average='macro'))\n",
        "print ('Naive - recall:', recall_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "\n",
        "# Linear Classifier\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"linear_classifier.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('Logistics Classifier, Accuracy =', np.mean(y_pred == y_test))\n",
        "print('Logistics - precision:', precision_score(y_test, y_pred, average='macro'))\n",
        "print('Logis - F1:', f1_score(y_test, y_pred, average='macro'))\n",
        "print ('LOgis - recall:', recall_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "\n",
        "# SVM\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"svm.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('SVM, Accuracy =', np.mean(y_pred == y_test))\n",
        "print('SVM - precision: ', precision_score(y_test, y_pred, average='macro'))\n",
        "print('SVM - F1:', f1_score(y_test, y_pred, average='macro'))\n",
        "print ('SVM - recall:', recall_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "\n",
        "# RNN\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"rnn.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print ('RNN, Accuracy =',np.mean(y_pred == y_test))\n",
        "print('RNN- precision:', precision_score(y_test, y_pred, average='macro'))\n",
        "print ('RNN - F1:', f1_score(y_test, y_pred, average='macro'))\n",
        "print ('RNN - recall:', recall_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "\n",
        "# Fasttext\n",
        "def print_results(N, p, r):\n",
        "    print(\"Fasttext, Precision = {}, Recall = {}\".format(p, r))\n",
        "    \n",
        "model = fasttext.load_model(os.path.join(MODEL_PATH,\"fasttext.ftz\"))\n",
        "print_results(*model.test('test.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QkJmLtgvIIz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "1dcec2e5-b462-432f-8643-1588f4ec27f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " __label__FR       0.86      1.00      0.93        19\n",
            " __label__NF       1.00      0.50      0.67         6\n",
            "\n",
            "    accuracy                           0.88        25\n",
            "   macro avg       0.93      0.75      0.80        25\n",
            "weighted avg       0.90      0.88      0.86        25\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Xem kết quả trên từng nhãn\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nb_model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
        "y_pred = nb_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=list(label_encoder.classes_)))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, labels=nb_model.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nb_model.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7krRjnQvNP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "4eafabcc-b28d-43e9-c5a8-f38d75b1a4ec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-744bc1d9d698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Phần mềm ứng dụng phải đầy đủ các chức năng cập nhật, điều chỉnh, hủy bỏ, tra cứu, thống kê, tìm kiếm theo các dữ liệu đã được thiết lập\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_preprocess' is not defined"
          ]
        }
      ],
      "source": [
        "# xem kết quả cho 1 văn bản model naive bayes đã load ở trên\n",
        "\n",
        "document = \"Phần mềm ứng dụng phải đầy đủ các chức năng cập nhật, điều chỉnh, hủy bỏ, tra cứu, thống kê, tìm kiếm theo các dữ liệu đã được thiết lập\"\n",
        "\n",
        "document = text_preprocess(document)\n",
        "document = remove_stopwords(document)\n",
        "\n",
        "label = nb_model.predict([document])\n",
        "print('Predict label:', label_encoder.inverse_transform(label))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Software Requirement Classification - Vietnamese",
      "provenance": [],
      "authorship_tag": "ABX9TyMqxcDayhZtmS9KYPwgYWQJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}