{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhamct/Software-Requirement-Classification/blob/main/Software_Requirement_Classification_Vietnamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrFIlT9gy6uv"
      },
      "source": [
        "# Bài toán:\n",
        "Input: tập dữ liệu tiếng Việt <br>\n",
        "Output: phân loại (yêu cầu chức năng, yêu cầu phi chức năng) <br>\n",
        "Process: sử dụng bộ thư viện Scikitlearn để thực hiện các thuật toán học máy có giám sát:\n",
        "*   Logistic Regression\n",
        "*   Naive Bayes\n",
        "*   Support Vector Machine\n",
        "Nếu còn thời gian, thực hiện thuật toán Fasttext (https://fasttext.cc/) của Facebook\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjGdHmUu1ONy"
      },
      "source": [
        "# Tiền xử lý\n",
        "\n",
        "\n",
        "1.   Xóa HTML code(nếu có)\n",
        "2.   Chuẩn hóa bảng mã UNICODE (đưa về UNICODE tổ hợp dựng sẵn)\n",
        "3.   Chuẩn hóa kiểu gõ dấu tiếng Việt\n",
        "4.   Tách từ tiếng Việt\n",
        "5.   Đưa về văn bản viết thường\n",
        "6. Xóa kí tự đặc biệt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Anjukome2dYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15f2864-4c08-4f9b-c527-1874ecc05541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: underthesea-core==0.0.4_alpha.10 in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.0.4a10)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.63.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.3.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.1.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.9.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "neekoSM843vD"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "from underthesea import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hbtYnHmT237c"
      },
      "outputs": [],
      "source": [
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        " \n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
        "        '|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "dicchar = loaddicchar()\n",
        "\n",
        "# Hàm chuyển Unicode dựng sẵn về Unicde tổ hợp (phổ biến hơn)\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "        lambda x: dicchar[x.group()], txt)\n",
        "\n",
        "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
        "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
        "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
        "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
        "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
        "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
        "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
        "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
        "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
        "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
        "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
        "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
        "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
        "\n",
        "nguyen_am_to_ids = {}\n",
        "\n",
        "for i in range(len(bang_nguyen_am)):\n",
        "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
        "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
        "\n",
        "def chuan_hoa_dau_tu_tieng_viet(word):\n",
        "    if not is_valid_vietnam_word(word):\n",
        "        return word\n",
        "\n",
        "    chars = list(word)\n",
        "    dau_cau = 0\n",
        "    nguyen_am_index = []\n",
        "    qu_or_gi = False\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9:  # check qu\n",
        "            if index != 0 and chars[index - 1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5:  # check gi\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "            chars[index] = bang_nguyen_am[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            nguyen_am_index.append(index)\n",
        "    if len(nguyen_am_index) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = nguyen_am_to_ids.get(chars[1])\n",
        "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
        "            else:\n",
        "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
        "                else:\n",
        "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
        "            return ''.join(chars)\n",
        "        return word\n",
        "\n",
        "    for index in nguyen_am_index:\n",
        "        x, y = nguyen_am_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8:  # ê, ơ\n",
        "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
        "            # for index2 in nguyen_am_index:\n",
        "            #     if index2 != index:\n",
        "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
        "            #         chars[index2] = bang_nguyen_am[x][0]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(nguyen_am_index) == 2:\n",
        "        if nguyen_am_index[-1] == len(chars) - 1:\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
        "        else:\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    else:\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
        "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def is_valid_vietnam_word(word):\n",
        "    chars = list(word)\n",
        "    nguyen_am_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if nguyen_am_index == -1:\n",
        "                nguyen_am_index = index\n",
        "            else:\n",
        "                if index - nguyen_am_index != 1:\n",
        "                    return False\n",
        "                nguyen_am_index = index\n",
        "    return True\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
        "    \"\"\"\n",
        "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
        "        :param sentence:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
        "        # print(cw)\n",
        "        if len(cw) == 3:\n",
        "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
        "        words[index] = ''.join(cw)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def remove_html(txt):\n",
        "    return re.sub(r'<[^>]*>', '', txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZO7eXEJYQaku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692a3da4-321d-4faf-a677-8b458629e068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tất_cả các thiết_bị của doanh_nghiệp được quản_lý bằng cách tích_hợp code bar code qr code rfid lưu_trữ toàn_bộ thông_tin mã_hóa của từng thiết_bị được ghi_nhận ở các trạng_thái đang hoạt_động không hoạt_động trên hệ_thống\n"
          ]
        }
      ],
      "source": [
        "def text_preprocess(document):\n",
        "    # xóa html code\n",
        "    document = remove_html(document)\n",
        "    # chuẩn hóa unicode\n",
        "    document = convert_unicode(document)\n",
        "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
        "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
        "    # tách từ\n",
        "    document = word_tokenize(document, format=\"text\")\n",
        "    # đưa về lower\n",
        "    document = document.lower()\n",
        "    # xóa các ký tự không cần thiết\n",
        "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
        "    # xóa khoảng trắng thừa\n",
        "    document = re.sub(r'\\s+', ' ', document).strip()\n",
        "    return document\n",
        "\n",
        "document = \"\"\"\n",
        "Tất cả các thiết bị của doanh nghiệp, được quản lý bằng cách tích hợp code (bar code/ QR code/ RFID,...) lưu trữ toàn bộ thông tin mã hóa của từng thiết bị, được ghi nhận ở các trạng thái Đang hoạt động/ Không hoạt động trên hệ thống. \n",
        "\"\"\"\n",
        "document = text_preprocess(document)\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99OoJx_rv8n"
      },
      "source": [
        "## PHÂN LOẠI TEXT\n",
        "* Tải tập dữ liệu đã tiền xử lý về\n",
        "\n",
        "* Quan sát tập dữ liệu\n",
        "* Loại bỏ stopword\n",
        "* Xây dựng tập train/test\n",
        "* Phân loại văn bản sử dụng thuật toán Naive Bayes\n",
        "* Phân loại văn bản sử dụng thuật toán SVM\n",
        "* Phân loại văn bản sử dụng Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRswJ-KNr__X"
      },
      "outputs": [],
      "source": [
        "!wget -nc \"https://github.com/nhamct/Software-Requirement-Classification/blob/main/dataset.zip\"\n",
        "!unzip -n \"dataset.zip\"\n",
        "!head \"dataset.txt\"\n",
        "#cần zip file lại --> download về --> unzip thì mới ko bị lỗi HTML tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RpEdiNlBsm_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7612595c-dbae-4a9d-83e7-233f237280ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__label__FR 98\n",
            "__label__NF 27\n"
          ]
        }
      ],
      "source": [
        "# Thống kê số lượng data theo nhãn\n",
        "count = {}\n",
        "for line in open('dataset.txt'):\n",
        "    key = line.split()[0]\n",
        "    count[key] = count.get(key, 0) + 1\n",
        "\n",
        "for key in count:\n",
        "    print(key, count[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "c5WEu7P1syOZ"
      },
      "outputs": [],
      "source": [
        "# Thống kê các word xuất hiện ở tất cả các nhãn\n",
        "total_label = 18\n",
        "vocab = {}\n",
        "label_vocab = {}\n",
        "for line in open('dataset.txt'):\n",
        "    words = line.split()\n",
        "    # lưu ý từ đầu tiên là nhãn\n",
        "    label = words[0]\n",
        "    if label not in label_vocab:\n",
        "        label_vocab[label] = {}\n",
        "    for word in words[1:]:\n",
        "        label_vocab[label][word] = label_vocab[label].get(word, 0) + 1\n",
        "        if word not in vocab:\n",
        "            vocab[word] = set()\n",
        "        vocab[word].add(label)\n",
        "\n",
        "count = {}\n",
        "for word in vocab:\n",
        "    if len(vocab[word]) == total_label:\n",
        "        count[word] = min([label_vocab[x][word] for x in label_vocab])\n",
        "        \n",
        "sorted_count = sorted(count, key=count.get, reverse=True)\n",
        "for word in sorted_count[:100]:\n",
        "    print(word, count[word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NGuQ9k69s_6J"
      },
      "outputs": [],
      "source": [
        "# loại stopword khỏi dữ liệu\n",
        "# lưu file dùng về sau\n",
        "stopword = set()\n",
        "with open('stopwords.txt', 'w') as fp:\n",
        "    for word in sorted_count[:100]:\n",
        "        stopword.add(word)\n",
        "        fp.write(word + '\\n')\n",
        "    \n",
        "def remove_stopwords(line):\n",
        "    words = []\n",
        "    for word in line.strip().split():\n",
        "        if word not in stopword:\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "    \n",
        "    \n",
        "with open('dataset.prep', 'w') as fp:\n",
        "    for line in open('dataset.txt'):\n",
        "        line = remove_stopwords(line)\n",
        "        fp.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "w3nEh7rQtD7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77aca694-f513-4cef-ec60-f6251702a313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__label__FR', '__label__NF'] \n",
            "\n",
            "Line Manager/ Production Manager xóa Template bảo trì trên hệ thống. 0 \n",
            "\n",
            "Technician đặt lịch nhắc cho Nhiệm vụ bảo trì/ ngày/ giờ tùy chọn, bao gồm Task ID, Date, Time 0\n"
          ]
        }
      ],
      "source": [
        "# Chia tập train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "test_percent = 0.2\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "\n",
        "for line in open('dataset.prep'):\n",
        "    words = line.strip().split()\n",
        "    label.append(words[0])\n",
        "    text.append(' '.join(words[1:]))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=42)\n",
        "\n",
        "# Lưu train/test data\n",
        "# Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
        "with open('train.txt', 'w') as fp:\n",
        "    for x, y in zip(X_train, y_train):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "with open('test.txt', 'w') as fp:\n",
        "    for x, y in zip(X_test, y_test):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "# encode label\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "print(list(label_encoder.classes_), '\\n')\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "print(X_train[0], y_train[0], '\\n')\n",
        "print(X_test[0], y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WGVKaWIvthdb"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"models\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    os.makedirs(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhYnb4kztp2-"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7thMfaa7tnvn",
        "outputId": "731ef563-6c7b-4ff3-e3b0-320397fbb1a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Naive Bayes in 0.0068242549896240234 seconds.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Naive Bayes in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"naive_bayes.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oESE0P0qt0SH"
      },
      "source": [
        "### Linear Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW-PQ-eKt1XJ",
        "outputId": "d86e24c7-991c-4abb-f8f1-1943a770021f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Linear Classifier in 0.012636661529541016 seconds.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "    \n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression(solver='lbfgs', \n",
        "                                                multi_class='auto',\n",
        "                                                max_iter=10000))\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Linear Classifier in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"linear_classifier.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kek4R0ZuZEu"
      },
      "source": [
        "### Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g5jOTkMubHL",
        "outputId": "cc37a91e-60a4-4f7d-b6f6-af6873690ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training SVM in 0.010424137115478516 seconds.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SVC(gamma='scale'))\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training SVM in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"svm.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN"
      ],
      "metadata": {
        "id": "DlXoCBBFytsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "VGc0K4JmywuQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1))\n",
        "                    ])\n",
        "\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "train_time = time.time() - start_time\n",
        "print('Done training RNN in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"rnn.pkl\"), 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RfpC6z0yxal",
        "outputId": "447283c4-cb57-40db-9929-fccc3fd81c1d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training RNN in 0.01576542854309082 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDfwJDfTu5C6"
      },
      "source": [
        "### FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtlaHQMXu6ST",
        "outputId": "1ea9ea26-5446-4dd0-a689-d8d4917834dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed pandas-1.1.5\n",
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=5baa5d399cb014cee3fb40ac1e754c2461631ee35ab0b9844d7d4a468718ca42\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting fasttext==0.9.2\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.1-py2.py3-none-any.whl (211 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.21.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3129043 sha256=6183c4b1c4810723cb5d49d7463c0bda2ecaacfca9416560b6cf4caf8c1a01eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.1\n"
          ]
        }
      ],
      "source": [
        "# Cài đặt fastText cho Python\n",
        "!pip install pandas==1.1.5\n",
        "!pip install wget==3.2\n",
        "!pip install fasttext==0.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "asW4bHSAu9MS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf29a5a-54fe-4b95-8657-5d6657d5c35c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Fasttext in 1.1008780002593994 seconds.\n"
          ]
        }
      ],
      "source": [
        "import fasttext\n",
        "\n",
        "start_time = time.time()\n",
        "model = fasttext.train_supervised(\n",
        "                                input='train.txt',\n",
        "                                dim=100,\n",
        "                                epoch=5,\n",
        "                                lr=0.1,\n",
        "                                wordNgrams=2,\n",
        "                                label='__label__',\n",
        "                                minCount=5\n",
        ")\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Fasttext in', train_time, 'seconds.')\n",
        "\n",
        "# Compress model files with quantization\n",
        "model.quantize(input='train.txt', retrain=True)\n",
        "model.save_model(os.path.join(MODEL_PATH,\"fasttext.ftz\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaxdhWu_vAyE"
      },
      "source": [
        "## Đánh giá các mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sLrghLnvDXS",
        "outputId": "e8755e19-5a9f-4044-a951-d64cf8ef27fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes, Accuracy = 0.88\n",
            "Linear Classifier, Accuracy = 0.84\n",
            "SVM, Accuracy = 0.88\n",
            "RNN, Accuracy = 0.76\n",
            "Fasttext, Precision = 0.8, Recall = 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Naive Bayes\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('Naive Bayes, Accuracy =', np.mean(y_pred == y_test))\n",
        "\n",
        "# Linear Classifier\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"linear_classifier.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('Linear Classifier, Accuracy =', np.mean(y_pred == y_test))\n",
        "\n",
        "# SVM\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"svm.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('SVM, Accuracy =', np.mean(y_pred == y_test))\n",
        "\n",
        "# RNN\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"rnn.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print ('RNN, Accuracy =',np.mean(y_pred == y_test))\n",
        "\n",
        "# Fasttext\n",
        "def print_results(N, p, r):\n",
        "    print(\"Fasttext, Precision = {}, Recall = {}\".format(p, r))\n",
        "    \n",
        "model = fasttext.load_model(os.path.join(MODEL_PATH,\"fasttext.ftz\"))\n",
        "print_results(*model.test('test.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7QkJmLtgvIIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7463e7-a1b6-480a-c771-c88981a4d11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " __label__FR       0.86      1.00      0.93        19\n",
            " __label__NF       1.00      0.50      0.67         6\n",
            "\n",
            "    accuracy                           0.88        25\n",
            "   macro avg       0.93      0.75      0.80        25\n",
            "weighted avg       0.90      0.88      0.86        25\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Xem kết quả trên từng nhãn\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nb_model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
        "y_pred = nb_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=list(label_encoder.classes_)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b7krRjnQvNP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "4eafabcc-b28d-43e9-c5a8-f38d75b1a4ec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-744bc1d9d698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Phần mềm ứng dụng phải đầy đủ các chức năng cập nhật, điều chỉnh, hủy bỏ, tra cứu, thống kê, tìm kiếm theo các dữ liệu đã được thiết lập\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_preprocess' is not defined"
          ]
        }
      ],
      "source": [
        "# xem kết quả cho 1 văn bản model naive bayes đã load ở trên\n",
        "\n",
        "document = \"Phần mềm ứng dụng phải đầy đủ các chức năng cập nhật, điều chỉnh, hủy bỏ, tra cứu, thống kê, tìm kiếm theo các dữ liệu đã được thiết lập\"\n",
        "\n",
        "document = text_preprocess(document)\n",
        "document = remove_stopwords(document)\n",
        "\n",
        "label = nb_model.predict([document])\n",
        "print('Predict label:', label_encoder.inverse_transform(label))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Software Requirement Classification - Vietnamese",
      "provenance": [],
      "authorship_tag": "ABX9TyOk8Lv+8TgtaVBYyZOWywkE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}