{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhamct/SRC_VNese/blob/main/Software_Requirement_Classification_Vietnamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrFIlT9gy6uv"
      },
      "source": [
        "# Bài toán:\n",
        "Input: tập dữ liệu tiếng Việt <br>\n",
        "Output: phân loại (yêu cầu chức năng, yêu cầu phi chức năng) <br>\n",
        "Process: sử dụng bộ thư viện Scikitlearn để thực hiện các thuật toán học máy có giám sát:\n",
        "*   Logistic Regression\n",
        "*   Naive Bayes\n",
        "*   Support Vector Machine\n",
        "Nếu còn thời gian, thực hiện thuật toán Fasttext (https://fasttext.cc/) của Facebook\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjGdHmUu1ONy"
      },
      "source": [
        "# Tiền xử lý\n",
        "\n",
        "\n",
        "1.   Xóa HTML code(nếu có)\n",
        "2.   Chuẩn hóa bảng mã UNICODE (đưa về UNICODE tổ hợp dựng sẵn)\n",
        "3.   Chuẩn hóa kiểu gõ dấu tiếng Việt\n",
        "4.   Tách từ tiếng Việt\n",
        "5.   Đưa về văn bản viết thường\n",
        "6. Xóa kí tự đặc biệt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Anjukome2dYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9bb2f2a-fb98-4aac-adf7-72df6e87526e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-1.3.4-py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Collecting underthesea-core==0.0.4_alpha.10\n",
            "  Downloading underthesea_core-0.0.4_alpha.10-cp37-cp37m-manylinux2010_x86_64.whl (581 kB)\n",
            "\u001b[K     |████████████████████████████████| 581 kB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 56.8 MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 63.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Installing collected packages: unidecode, underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.7 underthesea-1.3.4 underthesea-core-0.0.4a10 unidecode-1.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neekoSM843vD"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "from underthesea import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbtYnHmT237c"
      },
      "outputs": [],
      "source": [
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        " \n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
        "        '|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "dicchar = loaddicchar()\n",
        "\n",
        "# Hàm chuyển Unicode dựng sẵn về Unicde tổ hợp (phổ biến hơn)\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "        lambda x: dicchar[x.group()], txt)\n",
        "\n",
        "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
        "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
        "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
        "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
        "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
        "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
        "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
        "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
        "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
        "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
        "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
        "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
        "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
        "\n",
        "nguyen_am_to_ids = {}\n",
        "\n",
        "for i in range(len(bang_nguyen_am)):\n",
        "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
        "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
        "\n",
        "def chuan_hoa_dau_tu_tieng_viet(word):\n",
        "    if not is_valid_vietnam_word(word):\n",
        "        return word\n",
        "\n",
        "    chars = list(word)\n",
        "    dau_cau = 0\n",
        "    nguyen_am_index = []\n",
        "    qu_or_gi = False\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9:  # check qu\n",
        "            if index != 0 and chars[index - 1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5:  # check gi\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "            chars[index] = bang_nguyen_am[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            nguyen_am_index.append(index)\n",
        "    if len(nguyen_am_index) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = nguyen_am_to_ids.get(chars[1])\n",
        "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
        "            else:\n",
        "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
        "                else:\n",
        "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
        "            return ''.join(chars)\n",
        "        return word\n",
        "\n",
        "    for index in nguyen_am_index:\n",
        "        x, y = nguyen_am_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8:  # ê, ơ\n",
        "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
        "            # for index2 in nguyen_am_index:\n",
        "            #     if index2 != index:\n",
        "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
        "            #         chars[index2] = bang_nguyen_am[x][0]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(nguyen_am_index) == 2:\n",
        "        if nguyen_am_index[-1] == len(chars) - 1:\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
        "        else:\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    else:\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
        "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def is_valid_vietnam_word(word):\n",
        "    chars = list(word)\n",
        "    nguyen_am_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if nguyen_am_index == -1:\n",
        "                nguyen_am_index = index\n",
        "            else:\n",
        "                if index - nguyen_am_index != 1:\n",
        "                    return False\n",
        "                nguyen_am_index = index\n",
        "    return True\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
        "    \"\"\"\n",
        "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
        "        :param sentence:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
        "        # print(cw)\n",
        "        if len(cw) == 3:\n",
        "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
        "        words[index] = ''.join(cw)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def remove_html(txt):\n",
        "    return re.sub(r'<[^>]*>', '', txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO7eXEJYQaku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ed0bc0-b2ca-49c6-e4f7-85d2ca4ac336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tất_cả các thiết_bị của doanh_nghiệp được quản_lý bằng cách tích_hợp code bar code qr code rfid lưu_trữ toàn_bộ thông_tin mã_hóa của từng thiết_bị được ghi_nhận ở các trạng_thái đang hoạt_động không hoạt_động trên hệ_thống\n"
          ]
        }
      ],
      "source": [
        "def text_preprocess(document):\n",
        "    # xóa html code\n",
        "    document = remove_html(document)\n",
        "    # chuẩn hóa unicode\n",
        "    document = convert_unicode(document)\n",
        "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
        "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
        "    # tách từ\n",
        "    document = word_tokenize(document, format=\"text\")\n",
        "    # đưa về lower\n",
        "    document = document.lower()\n",
        "    # xóa các ký tự không cần thiết\n",
        "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
        "    # xóa khoảng trắng thừa\n",
        "    document = re.sub(r'\\s+', ' ', document).strip()\n",
        "    return document\n",
        "\n",
        "document = \"\"\"\n",
        "Tất cả các thiết bị của doanh nghiệp, được quản lý bằng cách tích hợp code (bar code/ QR code/ RFID,...) lưu trữ toàn bộ thông tin mã hóa của từng thiết bị, được ghi nhận ở các trạng thái Đang hoạt động/ Không hoạt động trên hệ thống. \n",
        "\"\"\"\n",
        "document = text_preprocess(document)\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99OoJx_rv8n"
      },
      "source": [
        "## PHÂN LOẠI TEXT\n",
        "* Tải tập dữ liệu đã tiền xử lý về\n",
        "\n",
        "* Quan sát tập dữ liệu\n",
        "* Loại bỏ stopword\n",
        "* Xây dựng tập train/test\n",
        "* Phân loại văn bản sử dụng thuật toán Naive Bayes\n",
        "* Phân loại văn bản sử dụng thuật toán SVM\n",
        "* Phân loại văn bản sử dụng Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRswJ-KNr__X"
      },
      "outputs": [],
      "source": [
        "!wget -nc \"https://github.com/nhamct/SRC_VNese/blob/main/dataset.txt\"\n",
        "\n",
        "!head \"dataset.txt\"\n",
        "#cần zip file lại --> download về --> unzip thì mới ko bị lỗi HTML tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpEdiNlBsm_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c992cfff-7932-4f7d-de96-6fbcb61149c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__label__FR 98\n",
            "__label__NF 27\n"
          ]
        }
      ],
      "source": [
        "# Thống kê số lượng data theo nhãn\n",
        "count = {}\n",
        "for line in open('dataset.txt'):\n",
        "    key = line.split()[0]\n",
        "    count[key] = count.get(key, 0) + 1\n",
        "\n",
        "for key in count:\n",
        "    print(key, count[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5WEu7P1syOZ"
      },
      "outputs": [],
      "source": [
        "# Thống kê các word xuất hiện ở tất cả các nhãn\n",
        "total_label = 18\n",
        "vocab = {}\n",
        "label_vocab = {}\n",
        "for line in open('dataset.txt'):\n",
        "    words = line.split()\n",
        "    # lưu ý từ đầu tiên là nhãn\n",
        "    label = words[0]\n",
        "    if label not in label_vocab:\n",
        "        label_vocab[label] = {}\n",
        "    for word in words[1:]:\n",
        "        label_vocab[label][word] = label_vocab[label].get(word, 0) + 1\n",
        "        if word not in vocab:\n",
        "            vocab[word] = set()\n",
        "        vocab[word].add(label)\n",
        "\n",
        "count = {}\n",
        "for word in vocab:\n",
        "    if len(vocab[word]) == total_label:\n",
        "        count[word] = min([label_vocab[x][word] for x in label_vocab])\n",
        "        \n",
        "sorted_count = sorted(count, key=count.get, reverse=True)\n",
        "for word in sorted_count[:100]:\n",
        "    print(word, count[word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGuQ9k69s_6J"
      },
      "outputs": [],
      "source": [
        "# loại stopword khỏi dữ liệu\n",
        "# lưu file dùng về sau\n",
        "stopword = set()\n",
        "with open('stopwords.txt', 'w') as fp:\n",
        "    for word in sorted_count[:100]:\n",
        "        stopword.add(word)\n",
        "        fp.write(word + '\\n')\n",
        "    \n",
        "def remove_stopwords(line):\n",
        "    words = []\n",
        "    for word in line.strip().split():\n",
        "        if word not in stopword:\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "    \n",
        "    \n",
        "with open('dataset.prep', 'w') as fp:\n",
        "    for line in open('dataset.txt'):\n",
        "        line = remove_stopwords(line)\n",
        "        fp.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3nEh7rQtD7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb0a220-72e8-4460-e4d4-9953e77e3a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__label__FR', '__label__NF'] \n",
            "\n",
            "Line Manager/ Production Manager xóa Template bảo trì trên hệ thống. 0 \n",
            "\n",
            "Technician đặt lịch nhắc cho Nhiệm vụ bảo trì/ ngày/ giờ tùy chọn, bao gồm Task ID, Date, Time 0\n"
          ]
        }
      ],
      "source": [
        "# Chia tập train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "test_percent = 0.2\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "\n",
        "for line in open('dataset.prep'):\n",
        "    words = line.strip().split()\n",
        "    label.append(words[0])\n",
        "    text.append(' '.join(words[1:]))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=42)\n",
        "\n",
        "# Lưu train/test data\n",
        "# Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
        "with open('train.txt', 'w') as fp:\n",
        "    for x, y in zip(X_train, y_train):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "with open('test.txt', 'w') as fp:\n",
        "    for x, y in zip(X_test, y_test):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "# encode label\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "print(list(label_encoder.classes_), '\\n')\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "print(X_train[0], y_train[0], '\\n')\n",
        "print(X_test[0], y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGVKaWIvthdb"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"models\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    os.makedirs(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhYnb4kztp2-"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7thMfaa7tnvn",
        "outputId": "5aac915a-5379-4c22-fd80-23cf03628280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Naive Bayes in 0.020845651626586914 seconds.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Naive Bayes in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"naive_bayes.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oESE0P0qt0SH"
      },
      "source": [
        "### Linear Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW-PQ-eKt1XJ",
        "outputId": "c349e384-e0c1-43bb-e3ea-f8d667c3b2e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Linear Classifier in 0.01909351348876953 seconds.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "    \n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression(solver='lbfgs', \n",
        "                                                multi_class='auto',\n",
        "                                                max_iter=10000))\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Linear Classifier in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"linear_classifier.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kek4R0ZuZEu"
      },
      "source": [
        "### Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g5jOTkMubHL",
        "outputId": "16778db7-7828-4ceb-f262-b5b4f2c3f76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training SVM in 0.010899543762207031 seconds.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SVC(gamma='scale'))\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training SVM in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"svm.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDfwJDfTu5C6"
      },
      "source": [
        "### FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtlaHQMXu6ST",
        "outputId": "0402635e-23a5-4118-e7c0-cc1ffd28ec20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: pybind11 in /root/.local/lib/python3.7/site-packages (2.9.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.12.0)\n",
            "--2022-03-10 13:29:58--  https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/facebookresearch/fastText/zip/refs/tags/v0.9.2 [following]\n",
            "--2022-03-10 13:29:58--  https://codeload.github.com/facebookresearch/fastText/zip/refs/tags/v0.9.2\n",
            "Resolving codeload.github.com (codeload.github.com)... 192.30.255.121\n",
            "Connecting to codeload.github.com (codeload.github.com)|192.30.255.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘v0.9.2.zip’\n",
            "\n",
            "v0.9.2.zip              [  <=>               ]   4.17M  12.7MB/s    in 0.3s    \n",
            "\n",
            "2022-03-10 13:29:58 (12.7 MB/s) - ‘v0.9.2.zip’ saved [4369852]\n",
            "\n",
            "Processing /content/fastText-0.9.2\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: pybind11>=2.2 in /root/.local/lib/python3.7/site-packages (from fasttext==0.9.2) (2.9.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.21.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3130105 sha256=b9a85c3f33d78dc6532cfcea3d6d72a35a7f26fd08c466c6f1055408b55d2248\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/b3/c9/4ae793bec22dc8570ea6defb4ebb24d699f12b8c954db5f44f\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "  Attempting uninstall: fasttext\n",
            "    Found existing installation: fasttext 0.9.2\n",
            "    Uninstalling fasttext-0.9.2:\n",
            "      Successfully uninstalled fasttext-0.9.2\n",
            "Successfully installed fasttext-0.9.2\n"
          ]
        }
      ],
      "source": [
        "# Cài đặt fastText cho Python\n",
        "!pip3 install --user numpy scipy pybind11 cmake\n",
        "!wget -nc \"https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\" > /dev/null\n",
        "!unzip -n \"v0.9.2.zip\" > /dev/null\n",
        "!cd fastText-0.9.2 && pip3 install --user .\n",
        "!rm -rf v0.9.2.zip fastText-0.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asW4bHSAu9MS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "fbec8521-9d22-4a8d-8152-c05706369def"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-dd035aafaed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model = fasttext.train_supervised(\n\u001b[1;32m      5\u001b[0m                                 \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import fasttext\n",
        "\n",
        "start_time = time.time()\n",
        "model = fasttext.train_supervised(\n",
        "                                input='train.txt',\n",
        "                                dim=100,\n",
        "                                epoch=5,\n",
        "                                lr=0.1,\n",
        "                                wordNgrams=2,\n",
        "                                label='__label__',\n",
        "                                minCount=5\n",
        ")\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Fasttext in', train_time, 'seconds.')\n",
        "\n",
        "# Compress model files with quantization\n",
        "model.quantize(input='train.txt', retrain=True)\n",
        "model.save_model(os.path.join(MODEL_PATH,\"fasttext.ftz\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaxdhWu_vAyE"
      },
      "source": [
        "### Đánh giá các mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sLrghLnvDXS",
        "outputId": "596f06f4-c23e-47f8-e23c-5dcbf1b52094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes, Accuracy = 0.88\n",
            "Linear Classifier, Accuracy = 0.84\n",
            "SVM, Accuracy = 0.88\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Naive Bayes\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('Naive Bayes, Accuracy =', np.mean(y_pred == y_test))\n",
        "\n",
        "# Linear Classifier\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"linear_classifier.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('Linear Classifier, Accuracy =', np.mean(y_pred == y_test))\n",
        "\n",
        "# SVM\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"svm.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('SVM, Accuracy =', np.mean(y_pred == y_test))\n",
        "\n",
        "# Fasttext\n",
        "#def print_results(N, p, r):\n",
        "  #  print(\"Fasttext, Precision = {}, Recall = {}\".format(p, r))\n",
        "    \n",
        "#model = fasttext.load_model(os.path.join(MODEL_PATH,\"fasttext.ftz\"))\n",
        "#print_results(*model.test('test.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QkJmLtgvIIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810bbbc4-345c-4709-9ddc-39fe775baeb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " __label__FR       0.86      1.00      0.93        19\n",
            " __label__NF       1.00      0.50      0.67         6\n",
            "\n",
            "    accuracy                           0.88        25\n",
            "   macro avg       0.93      0.75      0.80        25\n",
            "weighted avg       0.90      0.88      0.86        25\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Xem kết quả trên từng nhãn\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nb_model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
        "y_pred = nb_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=list(label_encoder.classes_)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7krRjnQvNP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d40ef6af-42e6-49bb-b7a3-da6ecdc99daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict label: ['__label__FR']\n"
          ]
        }
      ],
      "source": [
        "# xem kết quả cho 1 văn bản model naive bayes đã load ở trên\n",
        "\n",
        "document = \"Manager  nhập tên và mật khẩu để đăng nhập\"\n",
        "\n",
        "document = text_preprocess(document)\n",
        "document = remove_stopwords(document)\n",
        "\n",
        "label = nb_model.predict([document])\n",
        "print('Predict label:', label_encoder.inverse_transform(label))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Software Requirement Classification - Vietnamese",
      "provenance": [],
      "authorship_tag": "ABX9TyNyyLVnvH0Z+8dTLT9ZkZdx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}